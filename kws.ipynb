{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347dcaf9-1b31-404c-8004-1a7eb8e9f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytorch-lightning --q --upgrade\n",
    "# %pip install torchmetrics --q --upgrade\n",
    "\n",
    "# %pip install wandb --q\n",
    "# %pip install einops --q\n",
    "# %pip install soundfile --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf3dc59-5d09-4f8f-8b25-d67e5f201c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio, torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import argparse\n",
    "import numpy as np\n",
    "import wandb\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.datasets.speechcommands import load_speechcommands_item\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafc70a-3c5d-4c41-828d-2b8894bbe38f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3a61d-54de-440b-afa9-cea3589cc4db",
   "metadata": {},
   "source": [
    "Custom dataset classes for unknown speech commands and silence samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d720cb-846d-4640-9284-1b84ff0ece03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilenceDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(SilenceDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self.paths))\n",
    "        filepath = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class UnknownDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(UnknownDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self._walker))\n",
    "        fileid = self._walker[index]\n",
    "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4a61b-e0e8-43e9-a44e-19b416e46a99",
   "metadata": {},
   "source": [
    "KWS DataModule Handles transformation of waveform to MEL spectrum and the turning the \"image\" into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2cbc8a3-a634-4f11-86ee-02b111008d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWSDataModule(LightningDataModule):\n",
    "    def __init__(self, path, batch_size=128, num_workers=0, n_fft=512, \n",
    "                 n_mels=128, win_length=None, hop_length=256, patch_num=16, class_dict={}, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.class_dict = class_dict\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                                download=True,\n",
    "                                                                subset='training')\n",
    "\n",
    "        silence_dataset = SilenceDataset(self.path)\n",
    "        unknown_dataset = UnknownDataset(self.path)\n",
    "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "                                                                \n",
    "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                              download=True,\n",
    "                                                              subset='validation')\n",
    "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                               download=True,\n",
    "                                                               subset='testing')                                                    \n",
    "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                              n_fft=self.n_fft,\n",
    "                                                              win_length=self.win_length,\n",
    "                                                              hop_length=self.hop_length,\n",
    "                                                              n_mels=self.n_mels,\n",
    "                                                              power=2.0),\n",
    "            torchaudio.transforms.AmplitudeToDB()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.prepare_data()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        mels = []\n",
    "        labels = []\n",
    "        wavs = []\n",
    "        for sample in batch:\n",
    "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "            if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "            elif waveform.shape[-1] > sample_rate:\n",
    "                waveform = waveform[:,:sample_rate]\n",
    "\n",
    "            # mel from power to db\n",
    "            mels.append(self.transform(waveform))\n",
    "            labels.append(torch.tensor(self.class_dict[label]))\n",
    "            wavs.append(waveform)\n",
    "\n",
    "        mels = torch.stack(mels)\n",
    "        mels = rearrange(mels, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        labels = torch.stack(labels)\n",
    "        wavs = torch.stack(wavs)\n",
    "   \n",
    "        return mels, labels, wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1657767-7689-4cac-a42a-587fd8c37fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e90c984-688b-4a0f-bdef-8a207c511564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58ed6d6-09ef-4711-a6a5-1ec02d862853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484177e5-f01b-4218-92fa-ed44e51a3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69d9923-f29b-4d2e-8de6-4ea47927abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96fde76a-b043-4b90-91d7-4ce44d08cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341e765-77c7-4838-b4c1-122b713720dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformers Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1459ff60-1d6a-4ea4-8688-97d4c95733ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6c1025-eba7-448a-8b69-41291af2bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # log 10 sample audio predictions from the first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 10\n",
    "            mels, labels, wavs = batch\n",
    "            preds = outputs[\"y_hat\"]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            wavs = torch.squeeze(wavs, dim=1)\n",
    "            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
    "            \n",
    "            sample_rate = pl_module.hparams.sample_rate\n",
    "            idx_to_class = pl_module.hparams.idx_to_class\n",
    "            \n",
    "            # log audio samples and predictions as a W&B Table\n",
    "            columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
    "                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
    "            wandb_logger.log_table(\n",
    "                key='Transformers on KWS using PyTorch Lightning',\n",
    "                columns=columns,\n",
    "                data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bb491-a3f9-4a7f-bc02-39e4b1f830a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8375b8ae-fe6d-492a-a5b9-3c4062613ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    \n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "   \n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "    \n",
    "    # model hyperparameters\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    \n",
    "    parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "    \n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    \n",
    "    parser.add_argument(\"--no-wandb\", default=False, action='store_true')\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c4597-bce5-4281-8eea-ce4720430777",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4140a7-9dd8-4d23-b79d-4e0fb83aff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 4\n",
      "Sequence length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhizon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/Keyword_Spotting_Transformers/wandb/run-20220519_123416-nk0tvaq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/khizon/pl-kws/runs/nk0tvaq8\" target=\"_blank\">copper-leaf-4</a></strong> to <a href=\"https://wandb.ai/khizon/pl-kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | Transformer      | 597 K \n",
      "1 | embed   | Linear           | 4.2 K \n",
      "2 | fc      | Linear           | 151 K \n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "753 K     Trainable params\n",
      "0         Non-trainable params\n",
      "753 K     Total params\n",
      "1.507     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  90%|████████▉ | 1402/1558 [01:20<00:08, 17.40it/s, loss=1.08, v_num=vaq8]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 1/156 [00:00<01:15,  2.07it/s]\u001b[A\n",
      "Epoch 0:  90%|█████████ | 1403/1558 [01:21<00:08, 17.24it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  90%|█████████ | 1404/1558 [01:21<00:08, 17.25it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  90%|█████████ | 1405/1558 [01:21<00:08, 17.26it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  90%|█████████ | 1406/1558 [01:21<00:08, 17.27it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  90%|█████████ | 1407/1558 [01:21<00:08, 17.28it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:   4%|▍         | 6/156 [00:00<00:11, 12.75it/s]\u001b[A\n",
      "Epoch 0:  90%|█████████ | 1408/1558 [01:21<00:08, 17.28it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  90%|█████████ | 1409/1558 [01:21<00:08, 17.29it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1410/1558 [01:21<00:08, 17.29it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:   6%|▌         | 9/156 [00:00<00:09, 16.01it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 1411/1558 [01:21<00:08, 17.29it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1412/1558 [01:21<00:08, 17.30it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1413/1558 [01:21<00:08, 17.31it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1414/1558 [01:21<00:08, 17.31it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:   8%|▊         | 13/156 [00:00<00:06, 20.58it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 1415/1558 [01:21<00:08, 17.31it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1416/1558 [01:21<00:08, 17.32it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1417/1558 [01:21<00:08, 17.33it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  10%|█         | 16/156 [00:00<00:06, 22.74it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 1418/1558 [01:21<00:08, 17.33it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1419/1558 [01:21<00:08, 17.34it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1420/1558 [01:21<00:07, 17.34it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████ | 1421/1558 [01:21<00:07, 17.35it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  13%|█▎        | 20/156 [00:01<00:05, 26.91it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████▏| 1422/1558 [01:21<00:07, 17.36it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████▏| 1423/1558 [01:21<00:07, 17.36it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████▏| 1424/1558 [01:22<00:07, 17.37it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  91%|█████████▏| 1425/1558 [01:22<00:07, 17.37it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  15%|█▌        | 24/156 [00:01<00:04, 27.60it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 1426/1558 [01:22<00:07, 17.38it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1427/1558 [01:22<00:07, 17.38it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1428/1558 [01:22<00:07, 17.39it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1429/1558 [01:22<00:07, 17.40it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  18%|█▊        | 28/156 [00:01<00:04, 29.52it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 1430/1558 [01:22<00:07, 17.40it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1431/1558 [01:22<00:07, 17.40it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1432/1558 [01:22<00:07, 17.41it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1433/1558 [01:22<00:07, 17.42it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  21%|██        | 32/156 [00:01<00:04, 30.69it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 1434/1558 [01:22<00:07, 17.42it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1435/1558 [01:22<00:07, 17.43it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1436/1558 [01:22<00:06, 17.44it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1437/1558 [01:22<00:06, 17.44it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  23%|██▎       | 36/156 [00:01<00:03, 32.90it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 1438/1558 [01:22<00:06, 17.45it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1439/1558 [01:22<00:06, 17.46it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1440/1558 [01:22<00:06, 17.46it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  92%|█████████▏| 1441/1558 [01:22<00:06, 17.47it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  26%|██▌       | 40/156 [00:01<00:03, 33.39it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1442/1558 [01:22<00:06, 17.47it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1443/1558 [01:22<00:06, 17.47it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1444/1558 [01:22<00:06, 17.48it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1445/1558 [01:22<00:06, 17.48it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  28%|██▊       | 44/156 [00:01<00:03, 30.70it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1446/1558 [01:22<00:06, 17.49it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1447/1558 [01:22<00:06, 17.49it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1448/1558 [01:22<00:06, 17.50it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1449/1558 [01:22<00:06, 17.50it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  31%|███       | 48/156 [00:01<00:03, 27.91it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1450/1558 [01:22<00:06, 17.50it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1451/1558 [01:22<00:06, 17.51it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1452/1558 [01:22<00:06, 17.51it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1453/1558 [01:22<00:05, 17.52it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  33%|███▎      | 52/156 [00:02<00:03, 28.93it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1454/1558 [01:22<00:05, 17.52it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1455/1558 [01:23<00:05, 17.52it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  93%|█████████▎| 1456/1558 [01:23<00:05, 17.53it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  35%|███▌      | 55/156 [00:02<00:03, 27.41it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▎| 1457/1558 [01:23<00:05, 17.53it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▎| 1458/1558 [01:23<00:05, 17.54it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▎| 1459/1558 [01:23<00:05, 17.54it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▎| 1460/1558 [01:23<00:05, 17.55it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  38%|███▊      | 59/156 [00:02<00:03, 27.75it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1461/1558 [01:23<00:05, 17.55it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1462/1558 [01:23<00:05, 17.55it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1463/1558 [01:23<00:05, 17.56it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  40%|███▉      | 62/156 [00:02<00:03, 28.26it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1464/1558 [01:23<00:05, 17.57it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1465/1558 [01:23<00:05, 17.57it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1466/1558 [01:23<00:05, 17.58it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1467/1558 [01:23<00:05, 17.58it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  42%|████▏     | 66/156 [00:02<00:03, 29.72it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1468/1558 [01:23<00:05, 17.59it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1469/1558 [01:23<00:05, 17.60it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1470/1558 [01:23<00:05, 17.60it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  94%|█████████▍| 1471/1558 [01:23<00:04, 17.61it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  45%|████▍     | 70/156 [00:02<00:02, 29.36it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1472/1558 [01:23<00:04, 17.61it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1473/1558 [01:23<00:04, 17.61it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1474/1558 [01:23<00:04, 17.62it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1475/1558 [01:23<00:04, 17.62it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  47%|████▋     | 74/156 [00:02<00:02, 29.45it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1476/1558 [01:23<00:04, 17.63it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1477/1558 [01:23<00:04, 17.63it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1478/1558 [01:23<00:04, 17.64it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▍| 1479/1558 [01:23<00:04, 17.64it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  50%|█████     | 78/156 [00:02<00:02, 31.34it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1480/1558 [01:23<00:04, 17.65it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1481/1558 [01:23<00:04, 17.65it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1482/1558 [01:23<00:04, 17.66it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1483/1558 [01:23<00:04, 17.66it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  53%|█████▎    | 82/156 [00:03<00:02, 30.33it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 1484/1558 [01:23<00:04, 17.67it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1485/1558 [01:24<00:04, 17.67it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1486/1558 [01:24<00:04, 17.68it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  95%|█████████▌| 1487/1558 [01:24<00:04, 17.68it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  55%|█████▌    | 86/156 [00:03<00:02, 30.21it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1488/1558 [01:24<00:03, 17.69it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1489/1558 [01:24<00:03, 17.69it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1490/1558 [01:24<00:03, 17.70it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1491/1558 [01:24<00:03, 17.71it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  58%|█████▊    | 90/156 [00:03<00:02, 32.31it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1492/1558 [01:24<00:03, 17.71it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1493/1558 [01:24<00:03, 17.72it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1494/1558 [01:24<00:03, 17.73it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1495/1558 [01:24<00:03, 17.73it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1496/1558 [01:24<00:03, 17.74it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  61%|██████    | 95/156 [00:03<00:01, 35.59it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1497/1558 [01:24<00:03, 17.75it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1498/1558 [01:24<00:03, 17.75it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▌| 1499/1558 [01:24<00:03, 17.75it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▋| 1500/1558 [01:24<00:03, 17.76it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  63%|██████▎   | 99/156 [00:03<00:01, 31.18it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 1501/1558 [01:24<00:03, 17.76it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▋| 1502/1558 [01:24<00:03, 17.77it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  96%|█████████▋| 1503/1558 [01:24<00:03, 17.78it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1504/1558 [01:24<00:03, 17.78it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  66%|██████▌   | 103/156 [00:03<00:01, 32.13it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1505/1558 [01:24<00:02, 17.78it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1506/1558 [01:24<00:02, 17.79it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1507/1558 [01:24<00:02, 17.79it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1508/1558 [01:24<00:02, 17.80it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  69%|██████▊   | 107/156 [00:03<00:01, 31.58it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1509/1558 [01:24<00:02, 17.80it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1510/1558 [01:24<00:02, 17.81it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1511/1558 [01:24<00:02, 17.82it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1512/1558 [01:24<00:02, 17.82it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  71%|███████   | 111/156 [00:04<00:01, 31.65it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1513/1558 [01:24<00:02, 17.82it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1514/1558 [01:24<00:02, 17.83it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1515/1558 [01:24<00:02, 17.84it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1516/1558 [01:24<00:02, 17.84it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  74%|███████▎  | 115/156 [00:04<00:01, 32.10it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1517/1558 [01:25<00:02, 17.85it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1518/1558 [01:25<00:02, 17.85it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  97%|█████████▋| 1519/1558 [01:25<00:02, 17.85it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1520/1558 [01:25<00:02, 17.86it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  76%|███████▋  | 119/156 [00:04<00:01, 31.27it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1521/1558 [01:25<00:02, 17.86it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1522/1558 [01:25<00:02, 17.87it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1523/1558 [01:25<00:01, 17.88it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1524/1558 [01:25<00:01, 17.88it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  79%|███████▉  | 123/156 [00:04<00:01, 30.64it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1525/1558 [01:25<00:01, 17.88it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1526/1558 [01:25<00:01, 17.89it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1527/1558 [01:25<00:01, 17.90it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1528/1558 [01:25<00:01, 17.90it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  81%|████████▏ | 127/156 [00:04<00:00, 29.62it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1529/1558 [01:25<00:01, 17.90it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1530/1558 [01:25<00:01, 17.91it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1531/1558 [01:25<00:01, 17.91it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1532/1558 [01:25<00:01, 17.92it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  84%|████████▍ | 131/156 [00:04<00:00, 31.37it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1533/1558 [01:25<00:01, 17.92it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  98%|█████████▊| 1534/1558 [01:25<00:01, 17.93it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▊| 1535/1558 [01:25<00:01, 17.94it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▊| 1536/1558 [01:25<00:01, 17.94it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  87%|████████▋ | 135/156 [00:04<00:00, 32.02it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 1537/1558 [01:25<00:01, 17.94it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▊| 1538/1558 [01:25<00:01, 17.95it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1539/1558 [01:25<00:01, 17.96it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1540/1558 [01:25<00:01, 17.96it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  89%|████████▉ | 139/156 [00:04<00:00, 30.22it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1541/1558 [01:25<00:00, 17.96it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1542/1558 [01:25<00:00, 17.97it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1543/1558 [01:25<00:00, 17.97it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1544/1558 [01:25<00:00, 17.98it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  92%|█████████▏| 143/156 [00:05<00:00, 31.40it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1545/1558 [01:25<00:00, 17.98it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1546/1558 [01:25<00:00, 17.99it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1547/1558 [01:25<00:00, 17.99it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1548/1558 [01:26<00:00, 18.00it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  94%|█████████▍| 147/156 [00:05<00:00, 31.88it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1549/1558 [01:26<00:00, 18.00it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0:  99%|█████████▉| 1550/1558 [01:26<00:00, 18.00it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1551/1558 [01:26<00:00, 18.00it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1552/1558 [01:26<00:00, 18.01it/s, loss=1.08, v_num=vaq8]\n",
      "Validation DataLoader 0:  97%|█████████▋| 151/156 [00:05<00:00, 30.47it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1553/1558 [01:26<00:00, 18.02it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1554/1558 [01:26<00:00, 18.03it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1555/1558 [01:26<00:00, 18.04it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1556/1558 [01:26<00:00, 18.05it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|█████████▉| 1557/1558 [01:26<00:00, 18.05it/s, loss=1.08, v_num=vaq8]\n",
      "Epoch 0: 100%|██████████| 1558/1558 [01:26<00:00, 18.06it/s, loss=1.08, v_num=vaq8, test_loss=0.930, test_acc=73.10]\n",
      "Epoch 0: 100%|██████████| 1558/1558 [01:26<00:00, 18.06it/s, loss=1.08, v_num=vaq8, test_loss=0.930, test_acc=73.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1402: 'test_acc' reached 73.11256 (best 73.11256), saving model to '/home/jupyter/Keyword_Spotting_Transformers/data/speech_commands/checkpoints/transformers-kws-best-acc.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1558/1558 [01:26<00:00, 18.03it/s, loss=1.08, v_num=vaq8, test_loss=0.930, test_acc=73.10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 172/172 [00:04<00:00, 36.37it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             71.05115509033203\n",
      "        test_loss           1.0040781497955322\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>test_acc</td><td>█▁</td></tr><tr><td>test_loss</td><td>▁█</td></tr><tr><td>trainer/global_step</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>71.05116</td></tr><tr><td>test_loss</td><td>1.00408</td></tr><tr><td>trainer/global_step</td><td>1402</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">copper-leaf-4</strong>: <a href=\"https://wandb.ai/khizon/pl-kws/runs/nk0tvaq8\" target=\"_blank\">https://wandb.ai/khizon/pl-kws/runs/nk0tvaq8</a><br/>Synced 6 W&B file(s), 2 media file(s), 42 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220519_123416-nk0tvaq8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "    \n",
    "    # make a dictionary from CLASSES to integers\n",
    "    CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "    if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                               path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "                               win_length=args.win_length, hop_length=args.hop_length,\n",
    "                               patch_num=args.patch_num, class_dict=CLASS_TO_IDX)\n",
    "    datamodule.setup()\n",
    "    \n",
    "    data = iter(datamodule.train_dataloader()).next()\n",
    "    patch_dim = data[0].shape[-1]\n",
    "    seqlen = data[0].shape[-2]\n",
    "    print(\"Embed dim:\", args.embed_dim)\n",
    "    print(\"Patch size:\", 32 // args.patch_num)\n",
    "    print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "    model = LitTransformer(num_classes=args.num_classes, lr=args.lr, epochs=args.max_epochs, \n",
    "                           depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                           patch_dim=patch_dim, seqlen=seqlen,)\n",
    "\n",
    "    # wandb is a great way to debug and visualize this model\n",
    "    wandb_logger = WandbLogger(project=\"pl-kws\")\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=os.path.join(args.path, \"checkpoints\"),\n",
    "        filename=\"transformers-kws-best-acc\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='test_acc',\n",
    "        mode='max',\n",
    "    )\n",
    "    idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      precision=args.precision,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger if not args.no_wandb else None,\n",
    "                      callbacks=[model_checkpoint, WandbCallback() if not args.no_wandb else None])\n",
    "    model.hparams.sample_rate = datamodule.sample_rate\n",
    "    model.hparams.idx_to_class = idx_to_class\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "    wandb.finish()\n",
    "    #trainer.save_checkpoint('../mnist/checkpoint.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafb49f9-4258-4e1f-b2b6-da1967541ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
