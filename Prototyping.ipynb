{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b99c58-a428-44c4-ab3b-e0f671999b61",
      "metadata": {
        "id": "37b99c58-a428-44c4-ab3b-e0f671999b61"
      },
      "outputs": [],
      "source": [
        "# %pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# %pip install pytorch-lightning --q --upgrade\n",
        "# %pip install torchmetrics --q --upgrade\n",
        "# %pip install wandb --q\n",
        "# %pip install einops --q\n",
        "# %pip install soundfile --q\n",
        "# %pip install librosa --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246fc675-c948-4d14-9c89-6a796aba2275",
      "metadata": {
        "id": "246fc675-c948-4d14-9c89-6a796aba2275"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio, torchvision\n",
        "import os\n",
        "import matplotlib.pyplot as plt \n",
        "import librosa\n",
        "import argparse\n",
        "import numpy as np\n",
        "import wandb\n",
        "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torchmetrics.functional import accuracy\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torchaudio.datasets.speechcommands import load_speechcommands_item\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cff9ea0-4dd1-4482-9538-9898e719c8de",
      "metadata": {
        "id": "3cff9ea0-4dd1-4482-9538-9898e719c8de"
      },
      "outputs": [],
      "source": [
        "class SilenceDataset(SPEECHCOMMANDS):\n",
        "    def __init__(self, root):\n",
        "        super(SilenceDataset, self).__init__(root, subset='training')\n",
        "        self.len = len(self._walker) // 35\n",
        "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
        "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = np.random.randint(0, len(self.paths))\n",
        "        filepath = self.paths[index]\n",
        "        waveform, sample_rate = torchaudio.load(filepath)\n",
        "        return waveform, sample_rate, \"silence\", 0, 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "class UnknownDataset(SPEECHCOMMANDS):\n",
        "    def __init__(self, root):\n",
        "        super(UnknownDataset, self).__init__(root, subset='training')\n",
        "        self.len = len(self._walker) // 35\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = np.random.randint(0, len(self._walker))\n",
        "        fileid = self._walker[index]\n",
        "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
        "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8e4821-6527-4604-b3f0-34655bc1840f",
      "metadata": {
        "id": "7c8e4821-6527-4604-b3f0-34655bc1840f"
      },
      "outputs": [],
      "source": [
        "class KWSDataModule(LightningDataModule):\n",
        "    def __init__(self, path, batch_size=128, n_fft=512, \n",
        "                 n_mels=128, win_length=None, hop_length=256, num_workers=0, class_dict={}, patch_num=4, \n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.n_fft = n_fft\n",
        "        self.n_mels = n_mels\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.class_dict = class_dict\n",
        "        self.patch_num = patch_num\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                                download=True,\n",
        "                                                                subset='training')\n",
        "\n",
        "        silence_dataset = SilenceDataset(self.path)\n",
        "        unknown_dataset = UnknownDataset(self.path)\n",
        "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
        "                                                                \n",
        "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                              download=True,\n",
        "                                                              subset='validation')\n",
        "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
        "                                                               download=True,\n",
        "                                                               subset='testing')                                                    \n",
        "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
        "                                                              n_fft=self.n_fft,\n",
        "                                                              win_length=self.win_length,\n",
        "                                                              hop_length=self.hop_length,\n",
        "                                                              n_mels=self.n_mels,\n",
        "                                                              power=2.0),\n",
        "            torchaudio.transforms.AmplitudeToDB()\n",
        "        ])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.prepare_data()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collate_fn\n",
        "        )\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        labels = []\n",
        "        wavs = []\n",
        "        mels = []\n",
        "        for sample in batch:\n",
        "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
        "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
        "            if waveform.shape[-1] < sample_rate:\n",
        "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
        "            elif waveform.shape[-1] > sample_rate:\n",
        "                waveform = waveform[:,:sample_rate]\n",
        "\n",
        "            # mel from power to db\n",
        "            mels.append(self.transform(waveform))\n",
        "            labels.append(torch.tensor(self.class_dict[label]))\n",
        "            wavs.append(waveform)\n",
        "\n",
        "        mels = torch.stack(mels)\n",
        "        mels = rearrange(mels, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
        "        labels = torch.stack(labels)\n",
        "        wavs = torch.stack(wavs)\n",
        "   \n",
        "        return mels, labels, wavs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daaa014d-7df7-46bd-8dbd-c4ab70968267",
      "metadata": {
        "id": "daaa014d-7df7-46bd-8dbd-c4ab70968267"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "      \n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
        "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
        "   \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
        "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "def init_weights_vit_timm(module: nn.Module):\n",
        "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif hasattr(module, 'init_weights'):\n",
        "        module.init_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0516b2ae-0a48-4868-bc38-23134894c595",
      "metadata": {
        "id": "0516b2ae-0a48-4868-bc38-23134894c595"
      },
      "outputs": [],
      "source": [
        "class LitTransformer(LightningModule):\n",
        "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
        "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
        "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
        "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
        "\n",
        "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init_weights_vit_timm(self)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear projection\n",
        "        x = self.embed(x)\n",
        "            \n",
        "        # Encoder\n",
        "        x = self.encoder(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        return loss\n",
        "    \n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        acc = accuracy(y_hat, y)\n",
        "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
        "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.test_step(batch, batch_idx)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        return self.test_epoch_end(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e0e9e2-d19f-49c5-806e-bdf7e065e261",
      "metadata": {
        "id": "74e0e9e2-d19f-49c5-806e-bdf7e065e261"
      },
      "outputs": [],
      "source": [
        "class WandbCallback(Callback):\n",
        "\n",
        "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
        "        # log 10 sample audio predictions from the first batch\n",
        "        if batch_idx == 0:\n",
        "            n = 10\n",
        "            mels, labels, wavs = batch\n",
        "            preds = outputs['y_hat']\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "            labels = labels.cpu().numpy()\n",
        "            preds = preds.cpu().numpy()\n",
        "            \n",
        "            wavs = torch.squeeze(wavs, dim=1)\n",
        "            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
        "            \n",
        "            sample_rate = pl_module.hparams.sample_rate\n",
        "            idx_to_class = pl_module.hparams.idx_to_class\n",
        "            \n",
        "            # log audio samples and predictions as a W&B Table\n",
        "            columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
        "            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
        "                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
        "            wandb_logger.log_table(\n",
        "                key='Transformer on KWS using PyTorch Lightning',\n",
        "                columns=columns,\n",
        "                data=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5762b82e-cbae-4c82-8ce1-b94e05b760a7",
      "metadata": {
        "id": "5762b82e-cbae-4c82-8ce1-b94e05b760a7"
      },
      "outputs": [],
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # model training hyperparameters\n",
        "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
        "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
        "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
        "\n",
        "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
        "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
        "\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
        "                        help='number of epochs to train (default: 30)')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='learning rate (default: 0.001)')\n",
        "    \n",
        "    # mel spectrogram parameters\n",
        "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
        "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
        "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
        "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
        "\n",
        "    # where dataset will be stored\n",
        "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
        "\n",
        "    # 35 keywords + silence + unknown\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
        "\n",
        "    # 16-bit fp model to reduce the size\n",
        "    parser.add_argument(\"--precision\", default=16)\n",
        "    parser.add_argument(\"--accelerator\", default='gpu')\n",
        "    parser.add_argument(\"--devices\", default=1)\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
        "\n",
        "    parser.add_argument(\"--no-wandb\", default=False, action='store_true')\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "    return args\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "        if num_channels > 1:\n",
        "            axes[c].set_ylabel(f'Channel {c+1}')\n",
        "        if xlim:\n",
        "            axes[c].set_xlim(xlim)\n",
        "        if ylim:\n",
        "            axes[c].set_ylim(ylim)\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbebc35-545a-4185-845b-57499bb21021",
      "metadata": {
        "id": "4bbebc35-545a-4185-845b-57499bb21021"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = get_args()\n",
        "    CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
        "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
        "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
        "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
        "    \n",
        "    # make a dictionary from CLASSES to integers\n",
        "    CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
        "\n",
        "    if not os.path.exists(args.path):\n",
        "        os.makedirs(args.path, exist_ok=True)\n",
        "    datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
        "                               path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
        "                               win_length=args.win_length, hop_length=args.hop_length, patch_num=args.patch_num,\n",
        "                               class_dict=CLASS_TO_IDX)\n",
        "    datamodule.setup()\n",
        "\n",
        "    data = iter(datamodule.train_dataloader()).next()\n",
        "    patch_dim = data[0].shape[-1]\n",
        "    seqlen = data[0].shape[-2]\n",
        "    print(\"Embed dim:\", args.embed_dim)\n",
        "    print(\"Patch size:\", 32 // args.patch_num)\n",
        "    print(\"Sequence length:\", seqlen)\n",
        "    patch_dim = data[0].shape[-1]\n",
        "    \n",
        "    model = LitTransformer(num_classes=args.num_classes, lr=args.lr, epochs=args.max_epochs, \n",
        "                           depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
        "                           patch_dim=patch_dim, seqlen=seqlen,)\n",
        "\n",
        "    # wandb is a great way to debug and visualize this model\n",
        "    wandb_logger = WandbLogger(project=\"pl-kws\", log_model=\"all\")\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        dirpath=os.path.join(args.path, \"checkpoints\"),\n",
        "        filename=\"transformer-kws-best-acc\",\n",
        "        save_top_k=1,\n",
        "        verbose=True,\n",
        "        monitor='test_acc',\n",
        "        mode='max',\n",
        "    )\n",
        "    idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n",
        "    trainer = Trainer(accelerator=args.accelerator,\n",
        "                      devices=args.devices,\n",
        "                      precision=args.precision,\n",
        "                      max_epochs=args.max_epochs,\n",
        "                      logger=wandb_logger if not args.no_wandb else None,\n",
        "                      callbacks=[model_checkpoint, WandbCallback() if not args.no_wandb else None])\n",
        "    \n",
        "    model.hparams.sample_rate = datamodule.sample_rate\n",
        "    model.hparams.idx_to_class = idx_to_class\n",
        "\n",
        "    trainer.fit(model, datamodule=datamodule)\n",
        "    trainer.test(model, datamodule=datamodule)\n",
        "    \n",
        "    wandb.finish()\n",
        "    trainer.save_checkpoint('../kws/checkpoint.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca9c905-a6b5-4448-8574-b227f6dadf87",
      "metadata": {
        "id": "6ca9c905-a6b5-4448-8574-b227f6dadf87"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "run = wandb.init(project=\"test\")\n",
        "artifact = run.use_artifact('khizon/pl-kws/model-23su08sp:v0', type='model')\n",
        "artifact_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15054556-cae5-4023-ae7a-573e9956f885",
      "metadata": {
        "id": "15054556-cae5-4023-ae7a-573e9956f885"
      },
      "outputs": [],
      "source": [
        "# https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\n",
        "model = model.load_from_checkpoint(os.path.join(artifact_dir, \"model.ckpt\"))\n",
        "model.eval()\n",
        "script = model.to_torchscript()\n",
        "\n",
        "# save for use in production environment\n",
        "model_path = os.path.join(args.path, \"checkpoints\",\n",
        "                          \"resnet18-kws-best-acc.pt\")\n",
        "torch.jit.save(script, model_path)\n",
        "\n",
        "# list wav files given a folder\n",
        "label = CLASSES[2:]\n",
        "label = np.random.choice(label)\n",
        "path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
        "path = os.path.join(path, label)\n",
        "wav_files = [os.path.join(path, f)\n",
        "             for f in os.listdir(path) if f.endswith('.wav')]\n",
        "# select random wav file\n",
        "wav_file = np.random.choice(wav_files)\n",
        "waveform, sample_rate = torchaudio.load(wav_file)\n",
        "if waveform.shape[-1] < sample_rate:\n",
        "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
        "elif waveform.shape[-1] > sample_rate:\n",
        "    waveform = waveform[:,:sample_rate]\n",
        "\n",
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
        "                                                 n_fft=args.n_fft,\n",
        "                                                 win_length=args.win_length,\n",
        "                                                 hop_length=args.hop_length,\n",
        "                                                 n_mels=args.n_mels,\n",
        "                                                 power=2.0)\n",
        "\n",
        "mel = ToTensor()(librosa.power_to_db(\n",
        "    transform(waveform).squeeze().numpy(), ref=np.max))\n",
        "mel = mel.unsqueeze(0)\n",
        "mel = rearrange(mel, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=8, p2=8)\n",
        "scripted_module = torch.jit.load(model_path)\n",
        "pred = torch.argmax(scripted_module(mel), dim=1)\n",
        "print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Prototyping.ipynb",
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m92",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m92"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 5
=======
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fc675-c948-4d14-9c89-6a796aba2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio, torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import librosa\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wandb\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.datasets.speechcommands import load_speechcommands_item\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff9ea0-4dd1-4482-9538-9898e719c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilenceDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(SilenceDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self.paths))\n",
    "        filepath = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class UnknownDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(UnknownDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self._walker))\n",
    "        fileid = self._walker[index]\n",
    "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e4821-6527-4604-b3f0-34655bc1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWSDataModule(LightningDataModule):\n",
    "    def __init__(self, path, batch_size=128, n_fft=512, \n",
    "                 n_mels=128, win_length=None, hop_length=256, num_workers=0, class_dict={}, patch_num=4, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.class_dict = class_dict\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                                download=True,\n",
    "                                                                subset='training')\n",
    "\n",
    "        silence_dataset = SilenceDataset(self.path)\n",
    "        unknown_dataset = UnknownDataset(self.path)\n",
    "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "                                                                \n",
    "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                              download=True,\n",
    "                                                              subset='validation')\n",
    "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                               download=True,\n",
    "                                                               subset='testing')                                                    \n",
    "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.transform = torch.nn.Sequential(\n",
    "            torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                              n_fft=self.n_fft,\n",
    "                                                              win_length=self.win_length,\n",
    "                                                              hop_length=self.hop_length,\n",
    "                                                              n_mels=self.n_mels,\n",
    "                                                              power=2.0),\n",
    "            torchaudio.transforms.AmplitudeToDB()\n",
    "            )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.prepare_data()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        labels = []\n",
    "        wavs = []\n",
    "        mels = []\n",
    "        for sample in batch:\n",
    "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "            if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "            elif waveform.shape[-1] > sample_rate:\n",
    "                waveform = waveform[:,:sample_rate]\n",
    "\n",
    "            # mel from power to db\n",
    "            # mels.append(ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max)))\n",
    "            mels.append(self.transform(waveform))\n",
    "            labels.append(torch.tensor(self.class_dict[label]))\n",
    "            wavs.append(waveform)\n",
    "\n",
    "        mels = torch.stack(mels)\n",
    "        # mels = rearrange(mels, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        labels = torch.stack(labels)\n",
    "        wavs = torch.stack(wavs)\n",
    "   \n",
    "        return mels, labels, wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa014d-7df7-46bd-8dbd-c4ab70968267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, img_size=224, patch_dim=16, seqlen=16 embed_dim=768):\n",
    "        self.patch_dim=patch_dim\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            # Break down the image into p1 x p2 patches\n",
    "            Rearrange('b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1 = patch_dim, s2=seqlen),\n",
    "            torch.nn.Linear(patch_dim * seqlen * in_channels, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.randn(1,1, embed_dim))\n",
    "        self.positions = torch.nn.Parameter(torch.randn((img_size // patch_dim)**2 + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.proj(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516b2ae-0a48-4868-bc38-23134894c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, img_size=224 **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = PatchEmbedding(in_channels=in_channels, patch_dim=patch_dim, seqlen=seqlen, embed_dim=embed_dim, img_size=img_size)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0e9e2-d19f-49c5-806e-bdf7e065e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # log 10 sample audio predictions from the first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 10\n",
    "            mels, labels, wavs = batch\n",
    "            preds = outputs['y_hat']\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            wavs = torch.squeeze(wavs, dim=1)\n",
    "            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
    "            \n",
    "            sample_rate = pl_module.hparams.sample_rate\n",
    "            idx_to_class = pl_module.hparams.idx_to_class\n",
    "            \n",
    "            # log audio samples and predictions as a W&B Table\n",
    "            columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
    "                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
    "            wandb_logger.log_table(\n",
    "                key='Transformer on KWS using PyTorch Lightning',\n",
    "                columns=columns,\n",
    "                data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762b82e-cbae-4c82-8ce1-b94e05b760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model training hyperparameters\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 30)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "    \n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "\n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "\n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=False, action='store_true')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbebc35-545a-4185-845b-57499bb21021",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "    \n",
    "    # make a dictionary from CLASSES to integers\n",
    "    CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "    if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path, exist_ok=True)\n",
    "    datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                               path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "                               win_length=args.win_length, hop_length=args.hop_length, patch_num=args.patch_num,\n",
    "                               class_dict=CLASS_TO_IDX)\n",
    "    datamodule.setup()\n",
    "\n",
    "    data = iter(datamodule.train_dataloader()).next()\n",
    "    patch_dim = data[0].shape[-1]\n",
    "    seqlen = data[0].shape[-2]\n",
    "    print(\"Embed dim:\", args.embed_dim)\n",
    "    print(\"Patch size:\", 32 // args.patch_num)\n",
    "    print(\"Sequence length:\", seqlen)\n",
    "    patch_dim = data[0].shape[-1]\n",
    "    \n",
    "    model = LitTransformer(num_classes=args.num_classes, lr=args.lr, epochs=args.max_epochs, \n",
    "                           depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                           patch_dim=patch_dim, seqlen=seqlen,)\n",
    "\n",
    "    # wandb is a great way to debug and visualize this model\n",
    "    wandb_logger = WandbLogger(project=\"pl-kws\", log_model=\"all\")\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=os.path.join(args.path, \"checkpoints\"),\n",
    "        filename=\"transformer-kws-best-acc\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='test_acc',\n",
    "        mode='max',\n",
    "    )\n",
    "    idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      precision=args.precision,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger if not args.no_wandb else None,\n",
    "                      callbacks=[model_checkpoint, WandbCallback() if not args.no_wandb else None])\n",
    "    \n",
    "    model.hparams.sample_rate = datamodule.sample_rate\n",
    "    model.hparams.idx_to_class = idx_to_class\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    trainer.test(model, datamodule=datamodule)\n",
    "    \n",
    "    wandb.finish()\n",
    "    trainer.save_checkpoint('../kws/checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9c905-a6b5-4448-8574-b227f6dadf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init(project=\"test\")\n",
    "artifact = run.use_artifact('khizon/pl-kws/model-23su08sp:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15054556-cae5-4023-ae7a-573e9956f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\n",
    "model = model.load_from_checkpoint(os.path.join(artifact_dir, \"model.ckpt\"))\n",
    "model.eval()\n",
    "script = model.to_torchscript()\n",
    "\n",
    "# save for use in production environment\n",
    "model_path = os.path.join(args.path, \"checkpoints\",\n",
    "                          \"resnet18-kws-best-acc.pt\")\n",
    "torch.jit.save(script, model_path)\n",
    "\n",
    "# list wav files given a folder\n",
    "label = CLASSES[2:]\n",
    "label = np.random.choice(label)\n",
    "path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
    "path = os.path.join(path, label)\n",
    "wav_files = [os.path.join(path, f)\n",
    "             for f in os.listdir(path) if f.endswith('.wav')]\n",
    "# select random wav file\n",
    "wav_file = np.random.choice(wav_files)\n",
    "waveform, sample_rate = torchaudio.load(wav_file)\n",
    "if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "elif waveform.shape[-1] > sample_rate:\n",
    "    waveform = waveform[:,:sample_rate]\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                 n_fft=args.n_fft,\n",
    "                                                 win_length=args.win_length,\n",
    "                                                 hop_length=args.hop_length,\n",
    "                                                 n_mels=args.n_mels,\n",
    "                                                 power=2.0)\n",
    "\n",
    "mel = ToTensor()(librosa.power_to_db(\n",
    "    transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "mel = mel.unsqueeze(0)\n",
    "mel = rearrange(mel, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=8, p2=8)\n",
    "scripted_module = torch.jit.load(model_path)\n",
    "pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
>>>>>>> 40a6ff975ca928adcec697954531ddbbc04e1993
}
